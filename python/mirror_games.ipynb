{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "42eea32459e97d1fc562e9b561f8a94575df71e230e4ccdec5d01957082d6a36"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model trained on unmirrored data to show how it is assymetric\n",
    "no_mirror_model = keras.models.load_model('model_01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Team A in slot 1 vs team B in slot 2:\n[[1.5204343  1.2641668  1.0912144  0.9385684  0.7884336 ]\n [1.1753037  1.0936733  0.86529243 0.7218606  0.731142  ]]\nTeam B in slot 1 vs team A in slot 2:\n[[1.1279236  1.1067467  0.796592   0.68875617 0.7446804 ]\n [1.5647084  1.1852388  1.123609   0.97567785 0.88403565]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from json_functions import make_player_list, create_example\n",
    "\n",
    "players_list = make_player_list('very-big.json')\n",
    "\n",
    "teamA = ['device', 'Xyp9x', 'Magisk', 'dupreeh', 'gla1ve']\n",
    "teamB = ['EliGE', 'Stewie2K', 'Grim', 'FalleN', 'NAF']\n",
    "\n",
    "example_AvB = create_example(team1=teamA, team2=teamB, players_list=players_list)\n",
    "print(\"Team A in slot 1 vs team B in slot 2:\")\n",
    "pred_AvB = no_mirror_model.predict(np.asarray([example_AvB]))\n",
    "print(pred_AvB.reshape(2, 5))\n",
    "\n",
    "example_BvA = create_example(team1=teamB, team2=teamA, players_list=players_list)\n",
    "print(\"Team B in slot 1 vs team A in slot 2:\")\n",
    "pred_BvA = no_mirror_model.predict(np.asarray([example_BvA]))\n",
    "print(pred_BvA.reshape(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean abslote error of team A: 0.057661615\nmean abslote error of team B: 0.03515934\n"
     ]
    }
   ],
   "source": [
    "# Now lets quantify how differently the ratings were predicted based on which team was in which slot\n",
    "teamA_slot1 = pred_AvB.reshape(2, 5)[0]\n",
    "teamA_slot2 = pred_BvA.reshape(2, 5)[1]\n",
    "\n",
    "teamB_slot1 = pred_BvA.reshape(2, 5)[0]\n",
    "teamB_slot2 = pred_AvB.reshape(2, 5)[1]\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae_teamA = mean_absolute_error(teamA_slot2, teamA_slot1)\n",
    "mae_teamB = mean_absolute_error(teamB_slot2, teamB_slot1)\n",
    "\n",
    "print(\"mean abslote error of team A:\", mae_teamA)\n",
    "print(\"mean abslote error of team B:\", mae_teamB)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Mirroring Games\n",
    "Now we will rebuild the dataframe, except for every game in the json file, we will create 2 games to train from, with the teams swapping spots "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_mirrored_games(file_to_use):\n",
    "    raw_dataset = pd.read_json(file_to_use)\n",
    "    processed_data = []\n",
    "\n",
    "    players_list = make_player_list(file_to_use)\n",
    "    numPlayers = len(players_list)\n",
    "\n",
    "    for index, game in raw_dataset.iterrows():\n",
    "\n",
    "        if len(game.team1) > 5 or len(game.team2) > 5:\n",
    "            continue\n",
    "        roster_team1 = [0] * numPlayers\n",
    "        roster_team2 = [0] * numPlayers\n",
    "        rating_vector = [0]*10\n",
    "\n",
    "        # populate the team 1 roster and performance vectors\n",
    "        for i, player in enumerate(game.team1):\n",
    "            player_index = players_list.index(player['name'])\n",
    "            roster_team1[player_index] = 1\n",
    "            rating_vector[i] = round(float(player['rating']), 3)\n",
    "        # populate the team 2 roster and performance vectors\n",
    "        for i, player in enumerate(game.team2):\n",
    "            player_index = players_list.index(player['name'])\n",
    "            roster_team2[player_index] = 1\n",
    "            rating_vector[i+5] = round(float(player['rating']), 3)\n",
    "        row = [roster_team1+roster_team2, rating_vector]\n",
    "        processed_data.append(row)\n",
    "        \n",
    "        # now flip the teams around and add that to the training data\n",
    "        rating_vector_mirrored = rating_vector[5:10]+rating_vector[0:5]\n",
    "        row_mirrored = [roster_team2+roster_team1, rating_vector_mirrored]\n",
    "        processed_data.append(row_mirrored)\n",
    "\n",
    "    return players_list, pd.DataFrame(processed_data, columns=['rosters vector',\n",
    "                                                               'rating vector'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(17488, 1068)\n(17488, 10)\n"
     ]
    }
   ],
   "source": [
    "players_list_2, df = create_df_mirrored_games('very-big.json')\n",
    "\n",
    "features, outputs = df[\"rosters vector\"], df['rating vector']\n",
    "\n",
    "features = pd.DataFrame(features.values.tolist(), index= df.index)\n",
    "outputs = pd.DataFrame(outputs.values.tolist(), index= df.index)\n",
    "\n",
    "print(np.shape(features))\n",
    "print(np.shape(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll train a model on the mirrored games\n",
    "# get the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(1068, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "\t# model.add(Dense(50, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mae', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs, n_outputs = features.shape[1], outputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n_inputs, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, outputs, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/4\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.5936\n",
      "Epoch 2/4\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2070\n",
      "Epoch 3/4\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1852\n",
      "Epoch 4/4\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.1768\n",
      "Evaluation on test set:\n",
      "181/181 [==============================] - 0s 2ms/step - loss: 0.1929\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, verbose=1, epochs=4, batch_size=500)\n",
    "print(\"Evaluation on test set:\")\n",
    "mae = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Team A in slot 1 vs team B in slot 2:\n",
      "[[1.497519   1.2167796  1.1184615  1.08505    0.8956812 ]\n",
      " [1.1729797  0.98609143 0.9398011  0.79466444 0.6785067 ]]\n",
      "Team B in slot 1 vs team A in slot 2:\n",
      "[[1.1979443  0.9588642  0.98878944 0.7377537  0.67680544]\n",
      " [1.4609585  1.2312431  1.1338887  0.96310514 0.91969156]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Team A in slot 1 vs team B in slot 2:\")\n",
    "pred_AvB_mirrored = model.predict(np.asarray([example_AvB]))\n",
    "print(pred_AvB_mirrored.reshape(2, 5))\n",
    "\n",
    "print(\"Team B in slot 1 vs team A in slot 2:\")\n",
    "pred_BvA_mirrored = model.predict(np.asarray([example_BvA]))\n",
    "print(pred_BvA_mirrored.reshape(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean abslote error of team A trained on mirrored games: 0.042 non-mirrored: 0.058\nmean abslote error of team B trained on mirrored games: 0.032 non-mirrored: 0.035\n"
     ]
    }
   ],
   "source": [
    "# Now lets quantify how differently the ratings were predicted based on which team was in which slot\n",
    "teamA_slot1_mir = pred_AvB_mirrored.reshape(2, 5)[0]\n",
    "teamA_slot2_mir = pred_BvA_mirrored.reshape(2, 5)[1]\n",
    "\n",
    "teamB_slot1_mir = pred_BvA_mirrored.reshape(2, 5)[0]\n",
    "teamB_slot2_mir = pred_AvB_mirrored.reshape(2, 5)[1]\n",
    "\n",
    "mae_teamA_mir = mean_absolute_error(teamA_slot2_mir, teamA_slot1_mir)\n",
    "mae_teamB_mir = mean_absolute_error(teamB_slot2_mir, teamB_slot1_mir)\n",
    "\n",
    "print(\"mean abslote error of team A trained on mirrored games:\", round(mae_teamA_mir, 3), \"non-mirrored:\", round(mae_teamA, 3) )\n",
    "print(\"mean abslote error of team B trained on mirrored games:\", round(mae_teamB_mir, 3), \"non-mirrored:\", round(mae_teamB, 3))"
   ]
  },
  {
   "source": [
    "It seems like mirroring the games slightly reduces this problem, but I should test on more than just one game"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "players_list == players_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.2996206283569336, 0.8413699269294739, 0.9585452079772949, 0.6270332932472229, 0.5554747581481934, 0.9675542712211609, 0.9180113077163696, 0.6275364756584167, 0.6720717549324036, 0.5016393661499023, 1.269755482673645, 0.9563808441162109, 0.9189533591270447, 0.7656739950180054, 0.5631893873214722, 1.1734142303466797, 1.1531709432601929, 1.1774002313613892, 0.7385519742965698, 0.7036744356155396]\n[1.1138906478881836, 0.8115992546081543, 0.751567006111145, 0.7906836271286011, 0.6118819713592529, 1.219137191772461, 1.0641460418701172, 1.0334954261779785, 0.7137414216995239, 0.6560619473457336, 1.1405861377716064, 0.8060382604598999, 1.0057042837142944, 0.6832747459411621, 0.7849017381668091, 1.1554176807403564, 1.2117640972137451, 1.0568759441375732, 0.8240067362785339, 0.8355814218521118]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_players= len(players_list)\n",
    "\n",
    "preds_AvB = []\n",
    "preds_BvA = []\n",
    "preds_AvB_mir = []\n",
    "preds_BvA_mir = []\n",
    "\n",
    "for i in range(0, 500):\n",
    "    teamA = []\n",
    "    teamB = []\n",
    "    for j in range(0, 5):\n",
    "        teamA.append( players_list[random.randint(0, num_players-1)] )\n",
    "        teamB.append( players_list[random.randint(0, num_players-1)] )\n",
    "    \n",
    "    AvB = create_example(teamA, teamB, players_list)\n",
    "    BvA = create_example(teamB, teamA, players_list)\n",
    "\n",
    "    pred_AvB = no_mirror_model.predict(np.asarray([AvB]))[0].tolist()\n",
    "    pred_BvA = no_mirror_model.predict(np.asarray([BvA]))[0].tolist()\n",
    "\n",
    "    pred_AvB_mirrored = model.predict(np.asarray([AvB]))[0].tolist()\n",
    "    pred_BvA_mirrored = model.predict(np.asarray([BvA]))[0].tolist()\n",
    "\n",
    "    preds_AvB.extend(pred_AvB)\n",
    "    preds_BvA.extend(pred_BvA[5:10]+pred_BvA[0:5])\n",
    "\n",
    "    preds_AvB_mir.extend(pred_AvB_mirrored)\n",
    "    preds_BvA_mir.extend(pred_BvA_mirrored[5:10]+pred_BvA_mirrored[0:5])\n",
    "\n",
    "err_mir = mean_absolute_error(preds_AvB_mir, preds_BvA_mir)\n",
    "err = mean_absolute_error(preds_AvB, preds_BvA)\n",
    "# print(\"error mirrored:\", err_mir)\n",
    "# print(\"error non-mirrored:\", err)\n",
    "\n",
    "print(preds_AvB[0:20])\n",
    "print(preds_BvA[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "error mirrored: 0.115\nerror non-mirrored: 0.127\n"
     ]
    }
   ],
   "source": [
    "print(\"error mirrored:\", round(err_mir, 3))\n",
    "print(\"error non-mirrored:\", round(err, 3))"
   ]
  },
  {
   "source": [
    "# Conclusion\n",
    "\n",
    "The error, which in this case is just the average sum of the differences in predictions when a matchup is in either order in the model.predict call, is smaller on the mirrored model. Ideally we'd want the predictions for either team to be the same no matter which order the teams go in the predict function. The mirrored data still makes the predictions more similar than non-mirrored, but not by much and I'm not sure why"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "error mirrored: 0.05474535962939262\nerror non-mirrored: 0.08323495674133301\n"
     ]
    }
   ],
   "source": [
    "preds_AvB = []\n",
    "preds_BvA = []\n",
    "preds_AvB_mir = []\n",
    "preds_BvA_mir = []\n",
    "\n",
    "for i in range(0, 100):\n",
    "\n",
    "    AvB = X_test.iloc[i].tolist()\n",
    "    BvA = X_test.iloc[i].tolist()[num_players : 2*num_players]+X_test.iloc[i].tolist()[0 : num_players]\n",
    "\n",
    "    pred_AvB = no_mirror_model.predict(np.asarray([AvB]))[0].tolist()\n",
    "    pred_BvA = no_mirror_model.predict(np.asarray([BvA]))[0].tolist()\n",
    "\n",
    "    pred_AvB_mirrored = model.predict(np.asarray([AvB]))[0].tolist()\n",
    "    pred_BvA_mirrored = model.predict(np.asarray([BvA]))[0].tolist()\n",
    "\n",
    "    preds_AvB.extend(pred_AvB)\n",
    "    preds_BvA.extend(pred_BvA[5:10]+pred_BvA[0:5])\n",
    "\n",
    "    preds_AvB_mir.extend(pred_AvB_mirrored)\n",
    "    preds_BvA_mir.extend(pred_BvA_mirrored[5:10]+pred_BvA_mirrored[0:5])\n",
    "\n",
    "err_mir = mean_absolute_error(preds_AvB_mir, preds_BvA_mir)\n",
    "err = mean_absolute_error(preds_AvB, preds_BvA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "error mirrored:     0.055\nerror non-mirrored: 0.083\n"
     ]
    }
   ],
   "source": [
    "print(\"error mirrored:    \", round(err_mir, 3))\n",
    "print(\"error non-mirrored:\", round(err, 3))"
   ]
  },
  {
   "source": [
    "The result turns out to more dramatic for \"realistic\" rather than randomly generated data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}